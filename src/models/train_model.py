# -*- coding: utf-8 -*-
"""KD_Proj_G2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bjea4QTqTmx9QC3nYZNdAKA_grWNr9Du

**Import Libraries**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.metrics import plot_confusion_matrix
from xgboost import XGBClassifier
import xgboost as xgb

"""**Load Data and Splitting**"""

df_train = pd.read_csv('/content/csv_result-abc.csv')

X = df_train.drop(['id', 'user_id'], axis=1)
Y = df_train['user_id']

#X = preprocessing(X)
# df_test = preprocessing(df_test)

# scaler = norm(X, df_test)

# X = scaler.transform(X)
# df_test = scaler.transform(df_test)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

X.shape

X.head()

X['Agg_Column'] = X[X.columns[:]].apply(
    lambda x: ','.join(x.dropna().astype(str)),axis=1)

X.drop(X.iloc[:, 1:2], inplace = True, axis = 1)
X.head()

X.to_csv(r'/content/consumer_data_agg.csv')

"""**Modelling Random Forest**"""

clf = RandomForestClassifier()
y_predict = clf.fit(X_train, Y_train).predict(X_test)
clf.score(X_test, Y_test)
print(classification_report(Y_test, y_predict))
print(confusion_matrix(Y_test, y_predict))

"""**Visualizations Random Forest**"""

fig, ax = plt.subplots(figsize=(20, 20))
plot_confusion_matrix(clf, X_test, Y_test, normalize='true', cmap=plt.cm.Blues, ax=ax)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
feature_imp = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)
import seaborn as sns
# %matplotlib inline
# Creating a bar plot
fig, ax = plt.subplots(figsize=(20, 20))
sns.barplot(x=feature_imp, y=feature_imp.index, ax=ax)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")

"""**Modelling XGBoost**"""

pip install xgboost

pip install graphviz

from xgboost import XGBClassifier
model = XGBClassifier(objective ='multi:softmax', colsample_bytree = 0.3, learning_rate = 0.1,
                alpha = 10)
y_predict = model.fit(X_train, Y_train).predict(X_test)
model.score(X_test, Y_test)
print(classification_report(Y_test, y_predict))
print(confusion_matrix(Y_test, y_predict))

"""**Visualizations XGBoost**"""

xgb.plot_tree(model,num_trees=0)
plt.rcParams['figure.figsize'] = [1000, 1000]
plt.show()

xgb.plot_importance(model)
plt.rcParams['figure.figsize'] = [10, 10]
plt.show()

fig, ax = plt.subplots(figsize=(20, 20))
plot_confusion_matrix(model, X_test, Y_test, normalize='true', cmap=plt.cm.Greens, ax=ax)
plt.show()

"""**===> *Pickle* Random Forest Classifier**"""

import pickle as pickle
pickle.dump(clf, open("RF.sav", 'wb'))